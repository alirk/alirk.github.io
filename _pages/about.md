---
permalink: /
title: "Ali Ramezani-Kebrya"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am an Associate Professor (with tenure) in the [Department of Informatics](https://www.mn.uio.no/ifi/english/index.html) at the [University of Oslo (UiO)](https://www.uio.no/english/), a Principal Investigator at the [Norwegian Center for Knowledge-driven Machine Learning (Integreat)](https://www.integreat.no/), and the [SFI Visual Intelligence](https://www.visual-intelligence.no/), and a member of the [European Laboratory for Learning and Intelligent Systems (ELLIS) Society](https://ellis.eu/). I serve as an Area Chair of NeurIPS and AISTATS and Action Editor for [Transactions on Machine Learning Research](https://jmlr.org/tmlr/index.html). I have received [FRIPRO Grant for Early Career Scientists](https://www.integreat.no/news/two-new-integreat-pis-awarded-fripro-grants-ingrid.html). 

Before joining UiO, I was a Senior Scientific Collaborator at [EPFL](https://www.epfl.ch/en/), working with [Prof. Volkan Cevher](https://people.epfl.ch/volkan.cevher?lang=en) in [Laboratory for Information and Inference Systems (LIONS)](https://www.epfl.ch/labs/lions/). Before joining LIONS, I was an [NSERC Postdoctoral Fellow](https://www.nserc-crsng.gc.ca/students-etudiants/pd-np/pdf-bp_eng.asp) at the [Vector Institute](https://vectorinstitute.ai/) in Toronto working with [Prof. Daniel M. Roy](http://danroy.org/). I received my Ph.D. from the [University of Toronto](https://www.utoronto.ca/) where I was very fortunate to be advised by [Prof. Ben Liang](https://www.comm.utoronto.ca/~liang/) and [Prof. Min Dong](https://sites.google.com/ontariotechu.net/dong?pli=1). 

My current research is focused on **understanding how the input data distribution is encoded within layers of neural networks** and developing **theoretical concepts** and **practical tools** to minimize the statistical risk under resource constraints and realistic settings referring to **statistical and system** characteristics contrary to an ideal learning setting. I am interested in a broad range of applications including but not limited to **emotion recognition**, **marine data**,  and **neuroscience**.

-----

Recent News
======  
- 7/2025: **I am hiring for two exciting positions:** [**PhD**](https://www.jobbnorge.no/en/available-jobs/job/283103/phd-research-fellow-in-machine-learning) and [**Postdoc**](https://www.jobbnorge.no/en/available-jobs/job/283109/postdoctoral-research-fellow-in-machine-learning)
- 5/2025: Our paper *[Quantized Optimistic Dual Averaging with Adaptive Layer-wise Compression](https://arxiv.org/abs/2505.14371)* has been accepted to **ICML 2025**.
- 4/2025: I have received a [**FRIPRO Grant for Early Career Scientists**](https://www.integreat.no/news/two-new-integreat-pis-awarded-fripro-grants-ingrid.html) for project **Machine Learning in Real World (MLReal)**.
- 1/2025: Our paper *[Addressing Label Shift in Distributed Learning via Entropy Regularization](https://openreview.net/forum?id=kuYxecnlv2)* has been accepted to **ICLR 2025**.
{% comment %}
- 1/2025: I have open Postdoc positions (3 year contract co-funded by EU under MSCA) [**Check Themes DSB4 and DSB3 here and email me if interested!**](https://www.uio.no/dscience/english/dstrain/research-areas2025/informatics/machine-learning-signal-processing-and-image-analy/)
{% endcomment %}
- 11/2024: I will serve as an Action Editor for [Transactions on Machine Learning Research](https://jmlr.org/tmlr/index.html).
- 10/2024: I will serve as an Area Chair of AISTATS 2025. 
- 8/2024: [Integreat](https://www.integreat.no/) is officially launched. 
- 4/2024: I have [**PhD Position in ML**](https://www.jobbnorge.no/en/available-jobs/job/262282/phd-research-fellow-in-machine-learning).
- 3/2024: I will serve as an Area Chair of NeurIPS 2024. 
- 2/2024: We have one [postdoc position](https://www.jobbnorge.no/en/available-jobs/job/255679/dstrain-msca-postdoctoral-fellowships-in-computational-and-natural-sciences-18-positions) in "Joint Physics-informed and Data-driven Complex Dynamical System Solvers".
- 2/2024: Our paper *[Mixed Nash for Robust Federated Learning](https://openreview.net/pdf?id=mqMzerrVOB)* has been accepted to the **Transactions on Machine Learning Research**.
- 2/2024: The Research Excellence Center [SFI Visual Intelligence](https://www.visual-intelligence.no/) has recognized our work [Federated Learning under Covariate Shifts with Generalization Guarantees](https://openreview.net/pdf?id=N7lCDaeNiS) as **Spotlight Publications** from 2023!   
- 1/2024: Our paper *[On the Generalization of Stochastic Gradient Descent with Momentum](https://jmlr.org/papers/v25/22-0068.html)* has been accepted to the **Journal of Machine Learning Research**.
{% comment %}
- 1/2024: We will have Multiple Exciting Ph.D. Positions through [Integreat](https://www.integreat.no/) joint with wonderful Computer Scientists, Statisticians, and Mathematicians across Norway!
- 1/2024: I am the main organizer of [Integreat](https://www.integreat.no/) **Discrepancies Seminar Series**.
- 1/2024: I am a Program Chair at the [Northern Lights Deep Learning Conference (NLDL 2024)](https://www.nldl.org/).
- 12/2024: Our new PhD research fellow  Amir Arfan has started the PhD through SUURPh programme. Welcome Amir!
- 10/2023: I gave a talk titled **Learning under Resource Constraints in Real World** at the  [University of Toronto](https://www.ece.utoronto.ca/).
- 9/2023: I gave a talk titled **Learning under Resource Constraints in Real World** at the  [University of Copenhagen](https://www.aicentre.dk/).
- 9/2023: I gave a talk titled **Learning under Resource Constraints in Real World** at [Integreat](https://www.integreat.no/).
- 6/2023: Our paper *[Federated Learning under Covariate Shifts with Generalization Guarantees](https://openreview.net/pdf?id=N7lCDaeNiS)* has been published in **Transactions on Machine Learning Research**.
- 4/2023: I am now a PI at the [Visual Intelligence](https://www.visual-intelligence.no/)!
- 3/2023: I gave a talk titled [Scalable and Robust Deep Learning](https://www.youtube.com/watch?v=kjaAfse9fzY&t=1120s&ab_channel=SFIVisualIntelligence) at the Visual Intelligence.
- 1/2023: We have an open [Ph.D. position](https://www.jobbnorge.no/en/available-jobs/job/239426/phd-research-fellow-in-machine-learning). Deadline: 28 Feb, 2023. 
- 1/2023: Our paper *[Distributed Extra-gradient with Optimal Complexity and Communication Guarantees](https://openreview.net/pdf?id=b3itJyarLM0)* has been accepted to **ICLR 2023**.
- 1/2023: I am now a member of the [ELLIS Society](https://ellis.eu/)!
- 1/2023: I joined the Department of Informatics at the University of Oslo!
- 12/2022: I gave a talk titled `Randomization Improves Deep Learning Security` at the Annual Workshop of the VILLUM Investigator Grant at Aalborg University. 
- 10/2022: Our paper *[MixTailor: Mixed Gradient Aggregation for Robust Learning Against Tailored Attacks](https://openreview.net/pdf?id=tqDhrbKJLS)* has been published in **Transactions on Machine Learning Research**. 
- 8/2022: I gave a talk titled `How Did DL Dominate Today’s ML? What Challenges and Limitations Remain? at the University of Oslo.
- 6/2022: I gave a talk titled `Scalable ML: Communication-efficiency, Security, and Architecture Design` at the University of Edinburgh.
- 2/2022: I gave a talk titled `Scalable ML: Communication-efficiency, Security, and Architecture Design` at the University of Liverpool.
- 09/2021: Our paper *[Subquadratic Overparameterization for Shallow Neural Networks](https://proceedings.neurips.cc/paper/2021/hash/5d9e4a04afb9f3608ccc76c1ffa7573e-Abstract.html)* has been accepted to **NeurIPS 2021**.
{% endcomment %}
-----

Selected Publications
======
<img style="float: left;" src="/images/LayerQuant.png" width="320"/> We have developed a general **layer-wise quantization** framework taking into account the statistical heterogeneity across layers and an efficient solver for distributed variational inequalities. We establish tight variance and code-length bounds for layer-wise quantization, which generalize the bounds for global quantization frameworks.  We empirically achieve up to a **150% speedup** over the baselines in end-to-end training time for training Wasserstein GAN on **12+ GPUs**.


Anh Duc Nguyen, Ilia Markov, Frank Zhengqing Wu, Ali Ramezani-Kebrya, Kimon Antonakopoulos, Dan Alistarh, and Volkan Cevher, **Layer-wise Quantization for Quantized Optimistic Dual Averaging**, ICML 2025. 
[arXiv](https://arxiv.org/abs/2505.14371){: .btn--research}

<img style="float: left;" src="/images/LS.png" width="320"/>  We have introduced a method to **minimize overall true risk** in distributed settings under inter-node and intra-node **label shifts**. We improve test-to-train density ratio estimation through Shannon entropy-based regularization. Our method encourages smoother and more reliable predictions that account for inherent uncertainty in the data with a better approximation of class conditional probabilities, and **achieves up to 20% improvement in average test error** in imbalanced settings approaching an upper bound.

Zhiyuan Wu\*, Changkyu Choi\*, Xiangcheng Cao, Volkan Cevher, and Ali Ramezani-Kebrya, **Addressing Label Shift in Distributed Learning via Entropy Regularization**, ICLR 2025.  
[pdf](https://openreview.net/pdf?id=kuYxecnlv2){: .btn--research} [code](https://github.com/zhiyuan-11/VRLS_main/tree/main){: .btn--research} [openreview](https://openreview.net/forum?id=kuYxecnlv2){: .btn--research} [arXiv](https://arxiv.org/abs/2502.02544){: .btn--research}


<img style="float: left;" src="/images/SGDEM.png" width="320"/>  We have shown it is crucial to establish an appropriate balance between the **optimization error** associated with the empirical risk and the **generalization error** when accelerating SGD with momentum and established generalization error bounds and explicit convergence rates for SGD with momentum under a broad range of hyperparameters including a general step-size rule. For smooth Lipschitz loss functions, we analyze SGD with early momentum (SGDEM) under a broad range
of step-sizes, and show that it can train machine learning models for multiple epochs with a guarantee for generalization.

Ali Ramezani-Kebrya, Kimon Antonakopoulos, Volkan Cevher, Ashish Khisti, and Ben Liang, **On the Generalization of Stochastic Gradient Descent with Momentum**, Journal of Machine Learning Research, vol. 25, pp. 1-56, Jan. 2024.  
[pdf](https://jmlr.org/papers/v25/22-0068.html){: .btn--research} [bib](https://jmlr.org/papers/v25/22-0068.bib){: .btn--research} [arXiv](https://arxiv.org/abs/1809.04564){: .btn--research}

<img style="float: left;" src="/images/FIDEM2.png" width="320"/>  Even for a single client, the distribution shift between training and test data, i.e., intra-client distribution shift,has been a major challenge for decades. For instance, scarce disease data for training and test in a local hospital can be different. We focus on the **overall generalization** performance on multiple clients  and modify the classical ERM to obtain an unbiased estimate of an overall true risk minimizer under **intra-client and inter-client covariate shifts**, develop an efficient density ratio estimation method under stringent privacy requirements of federated learning, and show importance-weighted ERM achieves smaller generalization error than classical ERM.

Ali Ramezani-Kebrya\*, Fanghui Liu\*, Thomas Pethick\*, Grigorios Chrysos, and Volkan Cevher, **Federated Learning under Covariate Shifts with Generalization Guarantees**, Transactions on Machine Learning Research, June 2023.  
[pdf](https://openreview.net/pdf?id=N7lCDaeNiS){: .btn--research} [code](https://github.com/LIONS-EPFL/Federated_Learning_Covariate_Shift_Code){: .btn--research} [openreview](https://openreview.net/forum?id=N7lCDaeNiS){: .btn--research} 

<img style="float: left;" src="/images/FID_evolution.png" width="350"/>  Beyond supervised learning, we accelerate large-scale monotone variational inequality problems with applications such as training GANs in  distributed settings. We propose **quantized generalized extra-gradient (Q-GenX) family of algorithms** with the optimal rate of convergence and achieve noticeable speedups when training GANs on multiple GPUs without performance degradation.  
 
Ali Ramezani-Kebrya\*, Kimon Antonakopoulos\*, Igor Krawczuk\*, Justin Deschenaux\*, and Volkan Cevher, **Distributed Extra-gradient with Optimal Complexity and Communication Guarantees**, ICLR 2023.  
[pdf](https://openreview.net/pdf?id=b3itJyarLM0){: .btn--research} [bib](https://scholar.googleusercontent.com/scholar.bib?q=info:0Rc5SDH6BLEJ:scholar.google.com/&output=citation&scisdr=Cpu_FFPiEKCS49Q_XLg:AJ9-iYsAAAAAZEk5RLiokHqP3Zwtjs3yxY4tW3g&scisig=AJ9-iYsAAAAAZEk5RPim2bqJoOMe3WMWc15niVM&scisf=4&ct=citation&cd=-1&hl=en){: .btn--research} [code](https://github.com/LIONS-EPFL/QGENX){: .btn--research} [openreview](https://openreview.net/forum?id=b3itJyarLM0){: .btn--research} 

<img style="float: left;" src="/images/MixTailor_Overview.png" width="350"/>   ML models are vulnerable to various attacks at training and test time including data/model poisoning and adversarial examples. We introduce MixTailor, a scheme based on randomization of the aggregation strategies that makes it impossible for the attacker to be fully informed.  **MixTailor: Mixed Gradient Aggregation for Robust Learning Against Tailored Attacks** increases computational complexity of designing tailored attacks for an informed adversary.

 
Ali Ramezani-Kebrya\*, Iman Tabrizian\*, Fartash Faghri, and Petar Popovski, **MixTailor: Mixed Gradient Aggregation for Robust Learning Against Tailored Attacks**, Transactions on Machine Learning Research, Oct. 2022.  
[pdf](https://openreview.net/pdf?id=tqDhrbKJLS){: .btn--research} [bib](https://www.jmlr.org/tmlr/papers/bib/tqDhrbKJLS.bib){: .btn--research} [code](https://github.com/Tabrizian/mix-tailor){: .btn--research} [arXiv](https://arxiv.org/abs/2207.07941){: .btn--research} [openreview](https://openreview.net/forum?id=tqDhrbKJLS){: .btn--research} 



<img style="float: left;" src="/images/OPNN.png" width="300"/>   Overparameterization refers to the important phenomenon where the width of a neural network is chosen such that learning algorithms can provably attain zero loss in nonconvex training. In **Subquadratic Overparameterization for Shallow Neural Networks**, we achieve the best known bounds on the number of parameters that is sufficient for gradient descent to converge to a global minimum with linear rate and probability approaching to one.

 
Chaehwan Song\*, Ali Ramezani-Kebrya\*, Thomas Pethick, Armin Eftekhari, and Volkan Cevher, **Subquadratic Overparameterization for Shallow Neural Networks**, NeurIPS 2021.  
[pdf](https://proceedings.neurips.cc/paper/2021/file/5d9e4a04afb9f3608ccc76c1ffa7573e-Paper.pdf){: .btn--research} [bib](https://scholar.googleusercontent.com/scholar.bib?q=info:kx3LBH3jDHQJ:scholar.google.com/&output=citation&scisdr=CgVA45jvEKCS5DjU-u4:AAGBfm0AAAAAY6XS4u5LcAezF3eXi9jM_VkuZB9hzmc2&scisig=AAGBfm0AAAAAY6XS4r70vs2W1lznoTFxd4JHdJ9kVKaF&scisf=4&ct=citation&cd=-1&hl=en){: .btn--research} [code](https://github.com/LIONS-EPFL/Subquadratic-Overparameterization){: .btn--research} [arXiv](https://arxiv.org/abs/2111.01875){: .btn--research} [openreview](https://openreview.net/forum?id=NhbFhfM960){: .btn--research} 


<img style="float: left;" src="/images/Result50_bs256.png" width="350"/>   In training deep models over multiple GPUs, the communication time required to share huge stochastic gradients is the main performance bottleneck. We closed the gap between theory and practice of unbiased gradient compression. **NUQSGD** is currently the method offering the highest communication-compression while still converging under regular (uncompressed) hyperparameter values.

 
Ali Ramezani-Kebrya, Fartash Faghri, Ilya Markov, Vitalii Aksenov, Dan Alistarh, and Daniel M. Roy, **NUQSGD: Provably Communication-Efficient Data-Parallel SGD via Nonuniform Quantization**, Journal of Machine Learning Research, vol. 22, pp. 1-43, Apr. 2021.  
[pdf](https://jmlr.org/papers/volume22/20-255/20-255.pdf){: .btn--research} [bib](https://www.jmlr.org/papers/v22/20-255.bib){: .btn--research} [code](https://github.com/fartashf/nuqsgd){: .btn--research} [arXiv](https://arxiv.org/abs/1908.06077){: .btn--research} 


<img style="float: left;" src="/images/MultiGPU.png" width="350"/>   Communication-efficient variants of SGD are often heuristic and fixed over the course of training. In **Adaptive Gradient Quantization for Data-Parallel SGD**, we empirically observe that the statistics of gradients of deep models change during the training and introduce two adaptive quantization schemes. We improve the validation accuracy by almost 2% on CIFAR-10 and 1% on ImageNet in challenging low-cost communication setups.  

 
Fartash Faghri\*, Iman Tabrizian\*, Ilya Markov, Dan Alistarh, Daniel M. Roy, and Ali Ramezani-Kebrya, **Adaptive Gradient Quantization for Data-Parallel SGD**, NeurIPS 2020.  
[pdf](https://papers.nips.cc/paper/2020/file/20b5e1cf8694af7a3c1ba4a87f073021-Paper.pdf){: .btn--research} [bib](https://scholar.googleusercontent.com/scholar.bib?q=info:xpAwoNIuzxUJ:scholar.google.com/&output=citation&scisdr=CgVA45jvEKCS5Djck8o:AAGBfm0AAAAAY6Xai8rnX4Rz-Zrxs7QCv6ocvm8RxOKV&scisig=AAGBfm0AAAAAY6Xaiy39O8cDh_0XnYOezqMyusWtK5Cu&scisf=4&ct=citation&cd=-1&hl=en){: .btn--research} [code](https://github.com/tabrizian/learning-to-quantize){: .btn--research} [arXiv](https://arxiv.org/abs/2010.12460){: .btn--research}

-----

Students 
======
- Sigurd Holmsen, Ph.D. in progress, University of Oslo.
- Daniel Kaiser, Ph.D. in progress, University of Tromsø.
- Johan Mylius Kroken, Ph.D. in progress, University of Tromsø. 
- Amir Arfan, Ph.D. in progress, University of Oslo.
- Zhiyuan Wu, Ph.D. in progress, University of Oslo.

- Frida Marie Engøy Westby, MS in progress, University of Oslo.
- Trude Halvorsen, MS in progress, University of Oslo.	
- Kjetil Karstensen Indrehus, MS in progress, University of Oslo.
- Issa Rashdan, MS in progress, University of Oslo.
- Arangan Subramaniam, MS in progress, University of Oslo.
- Preben Nicholai Castberg, MS in progress, University of Oslo.
- Truls de Lange, MS in progress, University of Oslo.

- Oskar Høgberg Simensen, summer intern, University of Oslo.
- Guri Marie Svenberg, summer intern, University of Oslo.
- Karen Stølan Nielsen, summer intern, University of Oslo.
	
- Co-supervision at the University of Toronto and EPFL  
	- Anh Duc Nguyen, undergraduate intern, EPFL.    
	- Thomas Michaelsen Pethick, Ph.D. in progress, EPFL.
	- Wanyun Xie, MSc KTH,  first job after graduation: Ph.D. at EPFL.
	- Fartash Faghri, Ph.D. UoT, first job after graduation: research scientist at Apple.
	- Iman Tabrizian, M.A.Sc. UoT, first job after graduation: full-time engineer at NVIDIA.
	
-----	

Major Collaborators
======

- [Dan Alistarh](https://people.csail.mit.edu/alistarh/)
- [Volkan Cevher](https://people.epfl.ch/volkan.cevher?lang=en)
- [Arnoldo Frigessi](https://www.med.uio.no/imb/english/people/aca/frigessi/)
- [Carsten Griwodz](https://www.mn.uio.no/ifi/english/people/aca/griff/) 
- [Robert Jenssen](https://en.uit.no/ansatte/person?p_document_id=41060)
- [Ole Christian Lingjærde](https://www.mn.uio.no/ifi/personer/vit/ole/)
- [Benjamin Ricaud](https://en.uit.no/ansatte/person?p_document_id=741975)
- [Sylvia Richardson](https://www.mrc-bsu.cam.ac.uk/people/in-alphabetical-order/n-to-s/sylvia-richardson/)
- [Daniel M. Roy](http://danroy.org/)
- [Kristoffer Wickstrøm](https://uit.no/ansatte/kristoffer.k.wickstrom)




